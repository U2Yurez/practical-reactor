```java
/**
 * Make use of ParallelFlux to branch out processing of events in such way that:
 * - filtering events that have metadata, printing out metadata, and mapping to json can be done in parallel.
 * Then branch in before appending events to store. `appendToStore` must be invoked sequentially!
 */
@Test
public void event_processor() {
  //todo: feel free to change code as you need
  Flux<String> eventStream = eventProcessor()
    .parallel()
    .runOn(Schedulers.parallel())
    .filter(event -> event.metaData.length() > 0)
    .doOnNext(event -> System.out.println("Mapping event: " + event.metaData))
    .map(this::toJson)
    .sequential()
    .concatMap(n -> appendToStore(n).thenReturn(n));

  //don't change code below
  StepVerifier.create(eventStream)
    .expectNextCount(250)
    .verifyComplete();

  List<String> steps = Scannable.from(eventStream)
    .parents()
    .map(Object::toString)
    .collect(Collectors.toList());

  String last = Scannable.from(eventStream)
    .steps()
    .collect(Collectors.toCollection(LinkedList::new))
    .getLast();

  Assertions.assertEquals("concatMap", last);
  Assertions.assertTrue(steps.contains("ParallelMap"), "Map operator not executed in parallel");
  Assertions.assertTrue(steps.contains("ParallelPeek"), "doOnNext operator not executed in parallel");
  Assertions.assertTrue(steps.contains("ParallelFilter"), "filter operator not executed in parallel");
  Assertions.assertTrue(steps.contains("ParallelRunOn"), "runOn operator not used");
}

private String toJson(Event n) {
  try {
    return new ObjectMapper().writeValueAsString(n);
  } catch (JsonProcessingException e) {
    throw Exceptions.propagate(e);
  }
}
```

У цьому тесті ви використовуєте `ParallelFlux` для розпаралелювання обробки подій, але гарантуєте, що операція запису подій у сховище (метод `appendToStore`) виконується послідовно.

### Ось як це працює:

1. **`eventProcessor().parallel()`**: Ви перетворюєте стандартний `Flux` у `ParallelFlux`, що дозволяє виконувати обробку подій у кількох потоках.

2. **`runOn(Schedulers.parallel())`**: Цей метод забезпечує, що обробка подій буде виконуватися на паралельних потоках. `Schedulers.parallel()` створює пул потоків, оптимізований для паралельної роботи.

3. **Операції паралельної обробки**:
    - **`filter(event -> event.metaData.length() > 0)`**: Події фільтруються, щоб залишити лише ті, що мають метадані. Ця операція також виконується паралельно.
    - **`doOnNext(event -> System.out.println("Mapping event: " + event.metaData))`**: Друк метаданих кожної події. Також виконується паралельно.
    - **`map(this::toJson)`**: Перетворення події на формат JSON. Ця операція теж розпаралелюється.

4. **`sequential()`**: Після виконання всіх операцій паралельно, за допомогою цього методу ви знову об'єднуєте потік у послідовний.

5. **`concatMap(n -> appendToStore(n).thenReturn(n))`**: Виклик `appendToStore()` для запису кожної події у сховище. `concatMap` гарантує, що ці операції будуть виконані послідовно, навіть якщо решта обробки була паралельною.

### Ключові моменти:
- **Паралельна обробка**: Фільтрація, виведення метаданих і перетворення в JSON виконуються паралельно завдяки використанню `ParallelFlux`.
- **Послідовний запис у сховище**: Операція `appendToStore` виконується послідовно завдяки використанню `concatMap`, що гарантує, що всі події будуть записані в тому порядку, в якому вони були отримані.

### Перевірка:
- Тест перевіряє, що всі 250 подій були оброблені.
- Використовується `Scannable` для перевірки, що деякі ключові оператори (наприклад, `ParallelMap`, `ParallelPeek`, `ParallelFilter`) виконувалися в паралельному режимі.

